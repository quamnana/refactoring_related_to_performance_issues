{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"final-first-research\"]\n",
    "collection = db[\"performance-refactorings\"]\n",
    "issues_collection = db[\"performance-issues\"]\n",
    "\n",
    "# List of types to consider\n",
    "types_to_include = [\n",
    "    \"Change Variable Type\",\n",
    "    \"Add Parameter\",\n",
    "    \"Move Class\",\n",
    "    \"Change Parameter Type\",\n",
    "    \"Rename Method\",\n",
    "    \"Add Method Annotation\",\n",
    "    \"Change Return Type\",\n",
    "    \"Rename Variable\",\n",
    "    \"Change Attribute Type\",\n",
    "    \"Extract Method\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and extract the Id column\n",
    "csv_file = \"../../data/refactoring_reasons.csv\"\n",
    "try:\n",
    "    csv_data = pd.read_csv(csv_file, encoding=\"latin1\")\n",
    "    omit_commit_ids = csv_data[\"commit_id\"].tolist()\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    csv_data = pd.DataFrame()\n",
    "    omit_commit_ids = []\n",
    "\n",
    "\n",
    "# Retrieve approximately 5 documents for each specified type from MongoDB excluding repo_name \"bsl-language-server\" and omit_ids\n",
    "sampled_docs = []\n",
    "for doc_type in types_to_include:\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$match\": {\n",
    "                \"repo_name\": {\"$ne\": \"bsl-language-server\"},\n",
    "                \"type\": doc_type,\n",
    "                \"_id\": {\"$nin\": omit_commit_ids},\n",
    "            }\n",
    "        },\n",
    "        {\"$sample\": {\"size\": 5}},\n",
    "    ]\n",
    "    sampled_docs.extend(list(collection.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the data\n",
    "def flatten_doc(doc, issue_data):\n",
    "    flattened = {\n",
    "        \"_id\": doc[\"_id\"],\n",
    "        \"commit_id\": doc[\"commit_id\"],\n",
    "        \"repo_name\": doc[\"repo_name\"],\n",
    "        \"type\": doc[\"type\"],\n",
    "        \"issue_title\": doc[\"issue_title\"],\n",
    "    }\n",
    "\n",
    "    # Add issue data to the flattened document\n",
    "    if issue_data:\n",
    "        repo_url = issue_data.get('repo_url')\n",
    "        flattened['issue_url'] = issue_data.get('issue_url')\n",
    "        flattened['repo_url'] = repo_url if repo_url else \"\"\n",
    "        flattened[\"commit_url\"] = f\"{repo_url}/commit/{doc['commit_id']}\" if repo_url else \"\"\n",
    "\n",
    "    # Flatten the first object in rightSideLocations\n",
    "    if doc[\"rightSideLocations\"]:\n",
    "        first_right_side = doc[\"rightSideLocations\"][0]\n",
    "        for key, value in first_right_side.items():\n",
    "            if key == \"filePath\":\n",
    "                flattened[key] =f\"{value}/{first_right_side['startLine']}-{first_right_side['endLine']}\" \n",
    "\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id                                 commit_id  \\\n",
      "0  665189cd4eb27c185fe55b7b  7b9efc2ee537d4417c9a59493ec79b3a66169cf1   \n",
      "1  667715563cbfb03cd0957472  9c1b6ad8e5587391bb090cf33f3b3e8cdd55b878   \n",
      "2  664e3d1b7b998c13cdb9ecbc  3a1260954b51e976834306c1b4163c527c88f877   \n",
      "3  664e3d1f7b998c13cdb9ecd0  1e19dada64ee3dde665473ac8a4b33a9e1f8efb8   \n",
      "4  664f70527b998c13cdbb7cad  176a3d215cd8adb855fb371e03e7d9aeee5118e2   \n",
      "\n",
      "      repo_name                   type  \\\n",
      "0        Payara  Add Method Annotation   \n",
      "1  vertx-ignite  Change Attribute Type   \n",
      "2       quarkus     Change Return Type   \n",
      "3       quarkus  Add Method Annotation   \n",
      "4      querydsl  Change Attribute Type   \n",
      "\n",
      "                                   issue_title  \\\n",
      "0      Upgrade to Hazelcast 3.5 for JCache fix   \n",
      "1                   Ignite cache cannot expire   \n",
      "2       Integrate Redis with the Quarkus Cache   \n",
      "3       Integrate Redis with the Quarkus Cache   \n",
      "4  querydsl-sql : Joda time should be optional   \n",
      "\n",
      "                                           issue_url  \\\n",
      "0        https://github.com/payara/Payara/issues/339   \n",
      "1  https://github.com/vert-x3/vertx-ignite/issues/95   \n",
      "2  https://github.com/quarkusio/quarkus/issues/27785   \n",
      "3  https://github.com/quarkusio/quarkus/issues/27785   \n",
      "4   https://github.com/querydsl/querydsl/issues/2025   \n",
      "\n",
      "                                  repo_url  \\\n",
      "0         https://github.com/payara/Payara   \n",
      "1  https://github.com/vert-x3/vertx-ignite   \n",
      "2     https://github.com/quarkusio/quarkus   \n",
      "3     https://github.com/quarkusio/quarkus   \n",
      "4     https://github.com/querydsl/querydsl   \n",
      "\n",
      "                                          commit_url  \\\n",
      "0  https://github.com/payara/Payara/commit/7b9efc...   \n",
      "1  https://github.com/vert-x3/vertx-ignite/commit...   \n",
      "2  https://github.com/quarkusio/quarkus/commit/3a...   \n",
      "3  https://github.com/quarkusio/quarkus/commit/1e...   \n",
      "4  https://github.com/querydsl/querydsl/commit/17...   \n",
      "\n",
      "                                            filePath  \\\n",
      "0  appserver/payara-appserver-modules/payara-jsr1...   \n",
      "1  src/main/java/io/vertx/spi/cluster/ignite/Igni...   \n",
      "2  extensions/cache/runtime/src/main/java/io/quar...   \n",
      "3  extensions/redis-cache/runtime/src/main/java/i...   \n",
      "4  querydsl-sql/src/main/java/com/querydsl/sql/ty...   \n",
      "\n",
      "                                  category                  use  \\\n",
      "0                     Caching Optimization  Performance-related   \n",
      "1                     Caching Optimization  Performance-related   \n",
      "2                     Caching Optimization  Performance-related   \n",
      "3                     Caching Optimization  Maintenance-related   \n",
      "4  Concurrency and Parallelism Enhancement  Performance-related   \n",
      "\n",
      "                                             reasons  \n",
      "0  This refactoring is directly aimed at improvin...  \n",
      "1  Adding the expiryPolicy attribute improves the...  \n",
      "2  The code provided implements a mechanism to re...  \n",
      "3  Adding the static keyword to the method and re...  \n",
      "4  Changing the attribute type to ThreadLocal<Sim...  \n"
     ]
    }
   ],
   "source": [
    "# Query issues collection and apply the flattening function\n",
    "flattened_docs = []\n",
    "for doc in sampled_docs:\n",
    "    issue_query = {\n",
    "        \"issue_number\": doc[\"issue_number\"],\n",
    "        \"issue_title\": doc[\"issue_title\"],\n",
    "        \"repo_fullname\": doc[\"repo_fullname\"],\n",
    "    }\n",
    "    issue_data = issues_collection.find_one(issue_query, {\"issue_url\": 1, \"repo_url\": 1})\n",
    "    flattened_doc = flatten_doc(doc, issue_data)\n",
    "    flattened_docs.append(flattened_doc)\n",
    "\n",
    "# Convert the list of flattened documents to a pandas DataFrame\n",
    "new_data_df = pd.DataFrame(flattened_docs)\n",
    "\n",
    "# Update the fetched issues in the original CSV data\n",
    "updated_csv_data = pd.concat([csv_data, new_data_df], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "updated_csv_data.to_csv(csv_file, index=False)\n",
    "\n",
    "print(updated_csv_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
